
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>VTA</title>
    <meta name="description" content="VTA">
    <meta name="author" content="">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://tvm.ai/assets/themes/custom-twitter/css/1.4.0/bootstrap.css" rel="stylesheet">
    <link href="https://tvm.ai/assets/themes/custom-twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">

    <!-- Le fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  <link rel="shortcut icon" href="images/logo/tvm-logo.png">
  -->
  <link href="/images/logo/tvm-logo-square.png" rel="icon" type="image/png"/>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-75982049-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}

    gtag('js', new Date());
    gtag('config', 'UA-75982049-2');
  </script>

</head>

  <body>
    <div class="topbar">
      <div class="fill">
        <div class="container">
          <h2 id="logo-wrap">
            <a href="/" class="nav">
              <img src="/images/logo/tvm-logo-small-black.png" width="100px">
            </a>
          </h2>
          <ul class="nav" id="nav-bar">
            
            
            



  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      
    
  
    
      
      	
      	<li><a href="https://tvm.ai/community">Community</a></li>
      	
      
      
    
  
    
      
      	
      	<li><a href="https://tvm.ai/about">About</a></li>
      	
      
      
    
  
    
      
      	
      	<li class="active"><a href="https://tvm.ai/vta" class="active">VTA</a></li>
      	
      
      
    
  
    
      
      
      	
      	<li><a href="https://tvm.ai/blog">Blog</a></li>
      	
      
    
  




            <li> <a href="https://sampl.cs.washington.edu/tvmconf">TVM Conference</a></li>
            <li> <a href="https://docs.tvm.ai/tutorials/">Tutorials</a></li>
            <li> <a href="https://docs.tvm.ai">Docs</a></li>
            <li> <a href="https://github.com/dmlc/tvm/">Github</a></li>
          </ul>
        </div>
      </div>
    </div>
    
<div class="container">
  <div class="content">
    <div class="row">
      <div class="span14">
        
<h1 id="about-vta">About VTA</h1>

<p>The Versatile Tensor Accelerator (VTA) is an extension of the TVM framework designed to advance deep learning and hardware innovation.
VTA is a programmable accelerator that exposes a RISC-like programming abstraction to describe compute and memory operations at the tensor level. We designed VTA to expose the most salient and common characteristics of mainstream deep learning accelerators, such as tensor operations, DMA load/stores, and explicit compute/memory arbitration.</p>

<p>VTA is more than a standalone accelerator design: itâ€™s an end-to-end solution that includes drivers, a JIT runtime, and an optimizing compiler stack based on TVM.
The current release includes a behavioral hardware simulator, as well as the infrastructure to deploy VTA on low-cost FPGA hardware for fast prototyping.
By extending the TVM stack with a customizable, and open source deep learning hardware accelerator design, we are exposing a transparent end-to-end deep learning stack from the high-level deep learning framework, down to the actual hardware design and implementation.
This forms a truly end-to-end, from software-to-hardware open source stack for deep learning systems.</p>

<p style="text-align: center"><img src="http://raw.githubusercontent.com/uwsampl/web-data/master/vta/blogpost/vta_stack.png" alt="image" width="50%" /></p>

<p>The VTA and TVM stack together constitute a blueprint for end-to-end, accelerator-centric deep learning system that can:</p>

<ul>
  <li>Provide an open deep learning system stack for hardware, compilers, and systems researchers alike to incorporate optimizations and co-design techniques.</li>
  <li>Lower the barrier of entry for machine learning practitioners to experiment with novel network architectures, operators and data representations that require specialized hardware support.</li>
</ul>

<p>VTA is a component of TVM which was a research project at the <a href="https://sampl.cs.washington.edu/">SAMPL group</a> of
Paul G. Allen School of Computer Science &amp; Engineering, University of Washington. The project is now driven by an open source community involving multiple industry and academic institutions.
The project adopts <a href="https://docs.tvm.ai/contribute/community.html">Apache-style merit based governace model</a>.</p>

<p>Read more about VTA in our <a href="https://tvm.ai/2018/07/12/vta-release-announcement.html">TVM blog post</a>, or in the <a href="https://arxiv.org/abs/1807.04188">VTA techreport</a>.</p>

      </div>
    </div>
  </div>
</div> <!-- /container -->


    



  </body>
</html>

